{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종본: 간력하게 함수로 보기 좋게 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#윈도우에서 열때\n",
    "options = Options()\n",
    "options.add_experimental_option('detach', True)  # 브라우저 바로 닫힘 방지\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])  # 불필요한 메시지 제거\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "driver.get('https://www.imdb.com/title/tt0898266/episodes/?ref_=tt_eps_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#리눅스 환경\n",
    "\n",
    "url = \"https://www.imdb.com/title/tt0898266/episodes/?ref_=tt_eps_sm\"\n",
    "driver = webdriver.Chrome(service=Service(\"/home/addinedu/dev_ws/driver/chromedriver-linux64/chromedriver\"))\n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "driver. get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req,'html.parser')  \n",
    "all_box = soup.find(class_= \"ipc-page-section ipc-page-section--base ipc-page-section--sp-pageMargin\")\n",
    "page_move = all_box.find(class_= 'ipc-tabs ipc-tabs--base ipc-tabs--align-left ipc-tabs--display-chip ipc-tabs--inherit')\n",
    "move_on = page_move.find_all(class_= 'ipc-tab ipc-tab-link ipc-tab--on-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫페이지로 이동\n",
    "page_2dong = driver.find_element(By.XPATH, f'//*[@id=\"__next\"]/main/div/section/div/section/div/div[1]/section[2]/section[1]/div[2]/ul/a[1]')\n",
    "page_2dong.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_episode_info(episode):\n",
    "    title = episode.find('div', class_=\"ipc-title__text\").get_text()\n",
    "    day = episode.find('span', class_=\"sc-f2169d65-10 iZXnmI\").get_text()\n",
    "    rating = episode.find('span', class_=\"ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb ratingGroup--imdb-rating\").get_text()\n",
    "\n",
    "    rating_value = float(re.search(r'(\\d+\\.\\d)', rating).group(1))\n",
    "\n",
    "    return title, day, rating_value\n",
    "\n",
    "# def navigate_to_page(driver, page_number):\n",
    "#     page_2dong = driver.find_element(By.XPATH, f'//*[@id=\"__next\"]/main/div/section/div/section/div/div[1]/section[2]/section[1]/div[2]/ul/a[{page_number}]')\n",
    "#     page_2dong.click()\n",
    "    \n",
    "def scrape_page(driver, page_number):\n",
    "    try:\n",
    "        try:\n",
    "            page_2dong = driver.find_element(By.XPATH, f'//*[@id=\"__next\"]/main/div/section/div/section/div/div[1]/section[2]/section[1]/div[2]/ul/a[{page_number}]')\n",
    "            page_2dong.click()\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"첫페이지 입니다 {page_number}: {e}\")\n",
    "        \n",
    "        req = driver.page_source\n",
    "        soup = BeautifulSoup(req, 'html.parser')  \n",
    "        all_box = soup.find(class_=\"ipc-page-section ipc-page-section--base ipc-page-section--sp-pageMargin\")\n",
    "        all_inf = all_box.find(class_='sc-7b9ed960-0 jNjsLo')\n",
    "        all_inf_2 = all_inf.find_all(class_='sc-282bae8e-1 dSEzwa episode-item-wrapper')\n",
    "\n",
    "        title_list = []\n",
    "        day_list = []\n",
    "        rating_list = []\n",
    "\n",
    "        for episode in all_inf_2:\n",
    "            title, day, rating_value = extract_episode_info(episode)\n",
    "            title_list.append(title)\n",
    "            day_list.append(day)\n",
    "            rating_list.append(rating_value)\n",
    "\n",
    "        combined_list = list(zip(title_list, day_list, rating_list))\n",
    "        return combined_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping page {page_number}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = len(move_on)  # Set the number of pages to move on\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(a):\n",
    "        result = scrape_page(driver, i+1)\n",
    "        if result:\n",
    "            all_data.extend(result)\n",
    "            \n",
    "    # navigate_to_page(driver, i+1)\n",
    "    \n",
    "    # Convert the list of tuples to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=['Title', 'Day', 'Rating'])\n",
    "\n",
    "    # Print or use the DataFrame as needed\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/gg/movie2.csv',sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\gg\\movie2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "df_sorted = df.sort_values(by='Rating', ascending=False)\n",
    "\n",
    "# Select the top 10 titles\n",
    "df_top10 = df_sorted.head(10)\n",
    "# df_bottom10 = df_sorted.tail(10)\n",
    "# Create a horizontal bar chart using Plotly Express\n",
    "fig = px.bar(df_top10, x='Rating', y='Title', orientation='h',\n",
    "             labels={'Rating': 'IMDb Rating', 'Title': 'Movie Title'},\n",
    "             title='Top 10 IMDb Ratings by Title',\n",
    "             height=600, width=800)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by='Rating', ascending=False)\n",
    "\n",
    "# Select the bottom 10 titles (lowest-rated) in ascending order\n",
    "df_top10 = df_sorted.head(10)\n",
    "\n",
    "# Create a box plot using Plotly Express for the bottom 10 titles\n",
    "fig = px.box(df_top10, y='Rating', title='IMDb Ratings Distribution (Head 10 Titles)',\n",
    "             labels={'Rating': 'IMDb Rating'})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x='Day', y='Rating', title='IMDb Ratings Over Time',\n",
    "              labels={'Rating': 'IMDb Rating', 'Day': 'Day'})\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 시즌별 회차별 평점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(a):\n",
    "    result = scrape_page(driver, i+1)\n",
    "    if result:\n",
    "        df = pd.DataFrame(result, columns=['Title', 'Day', 'Rating'])\n",
    "\n",
    "        \n",
    "        fig = px.bar(df, x='Title', y='Rating', title=f'IMDb Ratings - Page {i+1}',\n",
    "                     labels={'Rating': 'IMDb Rating'})\n",
    "        fig.update_layout(xaxis=dict(tickangle=45, tickmode='array', tickvals=list(df['Title'])))\n",
    "\n",
    "        # Save the plot as an HTML file (optional)\n",
    "        fig.write_html(f'imdb_ratings_page_{i+1}.html')\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "        sns.swarmplot(x=\"Day\", y=\"Rating\", data=df, color=\".5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
