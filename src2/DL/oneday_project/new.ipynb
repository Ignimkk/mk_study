{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The TensorFlow Datasets Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"EMNIST: Extending MNIST to handwritten letters.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "\n",
    "# EMNIST constants\n",
    "_EMNIST_URL = \"https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip\"\n",
    "\n",
    "_EMNIST_CITATION = \"\"\"\\\n",
    "@article{cohen_afshar_tapson_schaik_2017,\n",
    "    title={EMNIST: Extending MNIST to handwritten letters},\n",
    "    DOI={10.1109/ijcnn.2017.7966217},\n",
    "    journal={2017 International Joint Conference on Neural Networks (IJCNN)},\n",
    "    author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},\n",
    "    year={2017}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class EMNISTConfig(tfds.core.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for EMNIST CONFIG.\"\"\"\n",
    "\n",
    "    def __init__(self, *, class_number, train_examples, test_examples, **kwargs):\n",
    "        \"\"\"BuilderConfig for EMNIST class number.\n",
    "\n",
    "        Args:\n",
    "            class_number: There are six different splits provided in this dataset. And\n",
    "            have different class numbers.\n",
    "            train_examples: number of train examples\n",
    "            test_examples: number of test examples\n",
    "            **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super(EMNISTConfig, self).__init__(**kwargs)\n",
    "        self.class_number = class_number\n",
    "        self.train_examples = train_examples\n",
    "        self.test_examples = test_examples\n",
    "\n",
    "\n",
    "class EMNIST(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"EMNIST dataset.\"\"\"\n",
    "\n",
    "    URL = _EMNIST_URL\n",
    "    VERSION = tfds.core.Version(\"3.1.0\")\n",
    "    RELEASE_NOTES = {\n",
    "        \"3.0.0\": \"New split API (https://tensorflow.org/datasets/splits)\",\n",
    "        \"3.1.0\": \"Updated broken download URL\",\n",
    "    }\n",
    "    BUILDER_CONFIGS = [\n",
    "        EMNISTConfig(\n",
    "            name=\"byclass\",\n",
    "            class_number=62,\n",
    "            train_examples=697932,\n",
    "            test_examples=116323,\n",
    "            description=\"EMNIST ByClass\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"bymerge\",\n",
    "            class_number=47,\n",
    "            train_examples=697932,\n",
    "            test_examples=116323,\n",
    "            description=\"EMNIST ByMerge\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"balanced\",\n",
    "            class_number=47,\n",
    "            train_examples=112800,\n",
    "            test_examples=18800,\n",
    "            description=\"EMNIST Balanced\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"letters\",\n",
    "            class_number=37,\n",
    "            train_examples=88800,\n",
    "            test_examples=14800,\n",
    "            description=\"EMNIST Letters\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"digits\",\n",
    "            class_number=10,\n",
    "            train_examples=240000,\n",
    "            test_examples=40000,\n",
    "            description=\"EMNIST Digits\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"mnist\",\n",
    "            class_number=10,\n",
    "            train_examples=60000,\n",
    "            test_examples=10000,\n",
    "            description=\"EMNIST MNIST\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=(\n",
    "                \"The EMNIST dataset is a set of handwritten character digits \"\n",
    "                \"derived from the NIST Special Database 19 and converted to \"\n",
    "                \"a 28x28 pixel image format and dataset structure that directly \"\n",
    "                \"matches the MNIST dataset.\\n\\n\"\n",
    "                \"Note: Like the original EMNIST data, images provided here are \"\n",
    "                \"inverted horizontally and rotated 90 anti-clockwise. You can use \"\n",
    "                \"`tf.transpose` within `ds.map` to convert the images to a \"\n",
    "                \"human-friendlier format.\"\n",
    "            ),\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                \"image\": tfds.features.Image(shape=(28, 28, 1)),\n",
    "                \"label\": tfds.features.ClassLabel(\n",
    "                    num_classes=self.builder_config.class_number\n",
    "                ),\n",
    "            }),\n",
    "            supervised_keys=(\"image\", \"label\"),\n",
    "            homepage=(\n",
    "                \"https://www.nist.gov/itl/products-and-services/emnist-dataset\"\n",
    "            ),\n",
    "            citation=_EMNIST_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        filenames = {\n",
    "            \"train_data\": \"emnist-{}-train-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"train_labels\": \"emnist-{}-train-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"test_data\": \"emnist-{}-test-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"test_labels\": \"emnist-{}-test-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        dir_name = os.path.join(dl_manager.download_and_extract(self.URL), \"gzip\")\n",
    "        extracted = dl_manager.extract(\n",
    "            {k: os.path.join(dir_name, fname) for k, fname in filenames.items()}\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            tfds.core.SplitGenerator(\n",
    "                name=tfds.Split.TRAIN,\n",
    "                gen_kwargs=dict(\n",
    "                    num_examples=self.builder_config.train_examples,\n",
    "                    data_path=extracted[\"train_data\"],\n",
    "                    label_path=extracted[\"train_labels\"],\n",
    "                ),\n",
    "            ),\n",
    "            tfds.core.SplitGenerator(\n",
    "                name=tfds.Split.TEST,\n",
    "                gen_kwargs=dict(\n",
    "                    num_examples=self.builder_config.test_examples,\n",
    "                    data_path=extracted[\"test_data\"],\n",
    "                    label_path=extracted[\"test_labels\"],\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "\n",
    "def _extract_mnist_images(image_filepath, num_images):\n",
    "    with tf.io.gfile.GFile(image_filepath, \"rb\") as f:\n",
    "        f.read(16)  # header\n",
    "        buf = f.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(\n",
    "            buf,\n",
    "            dtype=np.uint8,\n",
    "        ).reshape(num_images, 28, 28, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def _extract_mnist_labels(labels_filepath, num_labels):\n",
    "    with tf.io.gfile.GFile(labels_filepath, \"rb\") as f:\n",
    "        f.read(8)  # header\n",
    "        buf = f.read(num_labels)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class EMNIST with abstract methods _info, _split_generators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m index, record\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# EMNIST 데이터셋 로드\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memnist/byclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# 데이터셋 정보 출력\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(info)\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:643\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    513\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m ):\n\u001b[1;32m    529\u001b[0m   \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m   dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[1;32m    651\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:498\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[0;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:202\u001b[0m, in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# First check whether we can find the corresponding dataset builder code\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m   \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m registered\u001b[38;5;241m.\u001b[39mDatasetNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Class not found\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:124\u001b[0m, in \u001b[0;36mbuilder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mregistered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimported_builder_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(Type[dataset_builder\u001b[38;5;241m.\u001b[39mDatasetBuilder], \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "File \u001b[0;32m~/venv/opencv2_venv/lib/python3.10/site-packages/tensorflow_datasets/core/registered.py:295\u001b[0m, in \u001b[0;36mimported_builder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _ABSTRACT_DATASET_REGISTRY:\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;66;03m# Will raise TypeError: Can't instantiate abstract class X with abstract\u001b[39;00m\n\u001b[1;32m    294\u001b[0m   \u001b[38;5;66;03m# methods y, before __init__ even get called\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m   \u001b[43m_ABSTRACT_DATASET_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-callable\u001b[39;00m\n\u001b[1;32m    296\u001b[0m   \u001b[38;5;66;03m# Alternatively, could manually extract the list of non-implemented\u001b[39;00m\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;66;03m# abstract methods.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is an abstract class.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class EMNIST with abstract methods _info, _split_generators"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EMNIST(tfds.core.GeneratorBasedBuilder):\n",
    "    # EMNIST 클래스의 내용을 여기에 붙여넣으세요.\n",
    "    URL = _EMNIST_URL\n",
    "    VERSION = tfds.core.Version(\"3.1.0\")\n",
    "    RELEASE_NOTES = {\n",
    "        \"3.0.0\": \"New split API (https://tensorflow.org/datasets/splits)\",\n",
    "        \"3.1.0\": \"Updated broken download URL\",\n",
    "    }\n",
    "    BUILDER_CONFIGS = [\n",
    "        EMNISTConfig(\n",
    "            name=\"byclass\",\n",
    "            class_number=62,\n",
    "            train_examples=697932,\n",
    "            test_examples=116323,\n",
    "            description=\"EMNIST ByClass\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"bymerge\",\n",
    "            class_number=47,\n",
    "            train_examples=697932,\n",
    "            test_examples=116323,\n",
    "            description=\"EMNIST ByMerge\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"balanced\",\n",
    "            class_number=47,\n",
    "            train_examples=112800,\n",
    "            test_examples=18800,\n",
    "            description=\"EMNIST Balanced\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"letters\",\n",
    "            class_number=37,\n",
    "            train_examples=88800,\n",
    "            test_examples=14800,\n",
    "            description=\"EMNIST Letters\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"digits\",\n",
    "            class_number=10,\n",
    "            train_examples=240000,\n",
    "            test_examples=40000,\n",
    "            description=\"EMNIST Digits\",\n",
    "        ),\n",
    "        EMNISTConfig(\n",
    "            name=\"mnist\",\n",
    "            class_number=10,\n",
    "            train_examples=60000,\n",
    "            test_examples=10000,\n",
    "            description=\"EMNIST MNIST\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=(\n",
    "                \"The EMNIST dataset is a set of handwritten character digits \"\n",
    "                \"derived from the NIST Special Database 19 and converted to \"\n",
    "                \"a 28x28 pixel image format and dataset structure that directly \"\n",
    "                \"matches the MNIST dataset.\\n\\n\"\n",
    "                \"Note: Like the original EMNIST data, images provided here are \"\n",
    "                \"inverted horizontally and rotated 90 anti-clockwise. You can use \"\n",
    "                \"`tf.transpose` within `ds.map` to convert the images to a \"\n",
    "                \"human-friendlier format.\"\n",
    "            ),\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                \"image\": tfds.features.Image(shape=(28, 28, 1)),\n",
    "                \"label\": tfds.features.ClassLabel(\n",
    "                    num_classes=self.builder_config.class_number\n",
    "                ),\n",
    "            }),\n",
    "            supervised_keys=(\"image\", \"label\"),\n",
    "            homepage=(\n",
    "                \"https://www.nist.gov/itl/products-and-services/emnist-dataset\"\n",
    "            ),\n",
    "            citation=_EMNIST_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        filenames = {\n",
    "            \"train_data\": \"emnist-{}-train-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"train_labels\": \"emnist-{}-train-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"test_data\": \"emnist-{}-test-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "            \"test_labels\": \"emnist-{}-test-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        dir_name = os.path.join(dl_manager.download_and_extract(self.URL), \"gzip\")\n",
    "        extracted = dl_manager.extract(\n",
    "            {k: os.path.join(dir_name, fname) for k, fname in filenames.items()}\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            tfds.core.SplitGenerator(\n",
    "                name=tfds.Split.TRAIN,\n",
    "                gen_kwargs=dict(\n",
    "                    num_examples=self.builder_config.train_examples,\n",
    "                    data_path=extracted[\"train_data\"],\n",
    "                    label_path=extracted[\"train_labels\"],\n",
    "                ),\n",
    "            ),\n",
    "            tfds.core.SplitGenerator(\n",
    "                name=tfds.Split.TEST,\n",
    "                gen_kwargs=dict(\n",
    "                    num_examples=self.builder_config.test_examples,\n",
    "                    data_path=extracted[\"test_data\"],\n",
    "                    label_path=extracted[\"test_labels\"],\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(self, num_examples, data_path, label_path):\n",
    "        images = _extract_mnist_images(data_path, num_examples)\n",
    "        labels = _extract_mnist_labels(label_path, num_examples)\n",
    "        data = list(zip(images, labels))\n",
    "\n",
    "        # Using index as key since data is always loaded in same order.\n",
    "        for index, (image, label) in enumerate(data):\n",
    "            record = {\"image\": image, \"label\": label}\n",
    "            yield index, record\n",
    "\n",
    "# EMNIST 데이터셋 로드\n",
    "dataset, info = tfds.load('emnist/byclass', split='train', with_info=True)\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(info)\n",
    "\n",
    "# 데이터셋에서 샘플 이미지와 라벨 가져오기\n",
    "for example in dataset.take(5):  # 처음 5개의 샘플만 사용\n",
    "    image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
