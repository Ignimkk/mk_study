{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# 데이터 경로\n",
    "image_folder = \"/home/addinedu/dev_ws/data/O\"\n",
    "label_csv_path = \"/home/addinedu/dev_ws/data/0_labels.csv\"\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (32, 32))  # 이미지 크기를 32x32로 조정\n",
    "    image = image / 255.0  # 이미지를 0과 1 사이의 값으로 정규화\n",
    "    return image\n",
    "\n",
    "# 이미지 경로와 라벨을 가져와서 데이터셋 생성\n",
    "def create_dataset(image_folder, label_csv_path):\n",
    "    # 라벨 CSV 파일 로드\n",
    "    label_df = pd.read_csv(label_csv_path)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # 각 이미지에 대해 라벨을 매핑하여 이미지와 라벨 데이터 생성\n",
    "    for index, row in label_df.iterrows():\n",
    "        image_path = os.path.join(image_folder, row[\"Image_Path\"])\n",
    "        label = row[\"Label\"]\n",
    "        image = load_and_preprocess_image(image_path)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# 데이터셋 생성\n",
    "images, labels = create_dataset(image_folder, label_csv_path)\n",
    "\n",
    "# 클래스 개수 계산\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# 이미지 데이터와 라벨 데이터를 훈련 및 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addinedu/venv/opencv2_venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9074 - loss: 0.3323 - val_accuracy: 0.9975 - val_loss: 0.0592\n",
      "Epoch 2/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.0519 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 3/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0660 - val_accuracy: 0.9993 - val_loss: 0.0579\n",
      "Epoch 4/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0985 - val_accuracy: 0.9998 - val_loss: 0.0749\n",
      "Epoch 5/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0875 - val_accuracy: 0.9985 - val_loss: 0.0821\n",
      "Epoch 6/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.2104 - val_accuracy: 1.0000 - val_loss: 0.0199\n",
      "Epoch 7/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0643 - val_accuracy: 0.9975 - val_loss: 0.0787\n",
      "Epoch 8/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.1136 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 9/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0159 - val_accuracy: 1.0000 - val_loss: 0.0136\n",
      "Epoch 10/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9973 - loss: 0.0537 - val_accuracy: 0.9966 - val_loss: 0.2107\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# 데이터 경로\n",
    "X_image_folder = \"/home/addinedu/dev_ws/data/handwritten_dataset/train/x_new_train1000\"\n",
    "O_image_folder = \"/home/addinedu/dev_ws/data/images.cv_cda9n6fio287zp8fuwsivo/data/new_train1000\"\n",
    "X_label_csv_path = \"/home/addinedu/dev_ws/data/handwritten_dataset/train/X2_labels.csv\"\n",
    "O_label_csv_path = \"/home/addinedu/dev_ws/data/images.cv_cda9n6fio287zp8fuwsivo/data/new_train1000.csv\"\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (32, 32))  # 이미지 크기를 32x32로 조정\n",
    "    image = image / 255.0  # 이미지를 0과 1 사이의 값으로 정규화\n",
    "    return image\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_dataset(image_folder, label_csv_path):\n",
    "    # 라벨 CSV 파일 로드\n",
    "    label_df = pd.read_csv(label_csv_path)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # 각 이미지에 대해 라벨을 매핑하여 이미지와 라벨 데이터 생성\n",
    "    for index, row in label_df.iterrows():\n",
    "        image_path = os.path.join(image_folder, row[\"Image_Path\"])\n",
    "        label = row[\"Label\"]\n",
    "        image = load_and_preprocess_image(image_path)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# X 데이터셋 생성\n",
    "X_images, X_labels = create_dataset(X_image_folder, X_label_csv_path)\n",
    "\n",
    "# O 데이터셋 생성\n",
    "O_images, O_labels = create_dataset(O_image_folder, O_label_csv_path)\n",
    "\n",
    "# 클래스 개수 계산\n",
    "num_classes = len(np.unique(X_labels))\n",
    "\n",
    "# X 이미지와 라벨을 합치기\n",
    "images = np.concatenate((X_images, O_images), axis=0)\n",
    "labels = np.concatenate((X_labels, O_labels), axis=0)\n",
    "\n",
    "# 이미지 데이터와 라벨 데이터를 훈련 및 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=13)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# 모델 컴파일 시 옵티마이저를 adam_optimizer로 설정\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 다른 디렉토리에 훈련된 모델 저장\n",
    "model.save(\"/home/addinedu/dev_ws/DL/cnn1_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addinedu/venv/opencv2_venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addinedu/venv/opencv2_venv/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.6047 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6133 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6157 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6109 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6092 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6109 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6048 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6119 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6106 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6015 - loss: nan - val_accuracy: 0.6127 - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# 데이터 경로\n",
    "X_image_folder = \"/home/addinedu/dev_ws/data/handwritten_dataset/train/x_new_train1000\"\n",
    "O_image_folder = \"/home/addinedu/dev_ws/data/images.cv_cda9n6fio287zp8fuwsivo/data/new_train1000\"\n",
    "X_label_csv_path = \"/home/addinedu/dev_ws/data/handwritten_dataset/train/X2_labels.csv\"\n",
    "O_label_csv_path = \"/home/addinedu/dev_ws/data/images.cv_cda9n6fio287zp8fuwsivo/data/new_train1000.csv\"\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (32, 32))  # 이미지 크기를 32x32로 조정\n",
    "    image = image / 255.0  # 이미지를 0과 1 사이의 값으로 정규화\n",
    "    return image\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_dataset(image_folder, label_csv_path):\n",
    "    # 라벨 CSV 파일 로드\n",
    "    label_df = pd.read_csv(label_csv_path)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # 각 이미지에 대해 라벨을 매핑하여 이미지와 라벨 데이터 생성\n",
    "    for index, row in label_df.iterrows():\n",
    "        image_path = os.path.join(image_folder, row[\"Image_Path\"])\n",
    "        label = row[\"Label\"]\n",
    "        image = load_and_preprocess_image(image_path)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# X 데이터셋 생성\n",
    "X_images, _ = create_dataset(X_image_folder, X_label_csv_path)\n",
    "X_labels = np.zeros(len(X_images), dtype=int)  # X 이미지는 0으로 라벨링\n",
    "\n",
    "# O 데이터셋 생성\n",
    "O_images, _ = create_dataset(O_image_folder, O_label_csv_path)\n",
    "O_labels = np.ones(len(O_images), dtype=int)  # O 이미지는 1로 라벨링\n",
    "\n",
    "# 이미지 데이터와 라벨 데이터를 합치기\n",
    "images = np.concatenate((X_images, O_images), axis=0)\n",
    "labels = np.concatenate((X_labels, O_labels), axis=0)\n",
    "\n",
    "# 이미지 데이터와 라벨 데이터를 훈련 및 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, shuffle=True,test_size=0.2, random_state=13)\n",
    "\n",
    "# 클래스 개수 계산\n",
    "num_classes = len(np.unique(X_labels))\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.BatchNormalization(),  # 배치 정규화 추가\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
